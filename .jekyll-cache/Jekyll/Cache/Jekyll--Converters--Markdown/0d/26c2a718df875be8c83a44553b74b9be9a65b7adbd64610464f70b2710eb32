I"<blockquote>
  <p>2018年3月6日，Google Brain 团队在distill上发表了7人共同完成的文章 <a href="https://distill.pub/2018/building-blocks/">《The Building Blocks of Interpretability》</a>。两日后，Limber便迅速在Udacity上写了一篇文章来介绍Google Brain的这个最新成果：<a href="https://zhuanlan.zhihu.com/p/34378318">《再说深度学习是黑匣子，就把这篇文章互Ta脸上》</a>。能拆黑匣子当然很让人兴奋了，遂找来原文读一读，不知道读完之后会不会产生把它糊到自己脸上的冲动。</p>
</blockquote>

<h1 id="一个解释神经网络的用户接口">一个解释神经网络的用户接口</h1>
<p>目前，神经网络在很多方面的推断能力已经超过了人类。但是，我们不仅希望神经网络能推断以帮助我们决策，还希望它对做出的决策进行解释，即神经网络需要具备一定的可解释性(Interpretability)。Google Brain的这篇论文提供了一个用户接口，使神经网络能向“人”解释它看到了什么，以及它做出最终决定的依据。该接口整合了特征可视化、归因、以及降维等增强解释性的方法。</p>

:ET